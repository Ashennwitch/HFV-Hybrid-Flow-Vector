{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1afSWj8E0eQRWV4qtNt0K0tOtePK1YDnK","authorship_tag":"ABX9TyMBHzlUPZ5ka6btGsIhoMdJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install scapy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vdr0s8I472ht","executionInfo":{"status":"ok","timestamp":1762600117273,"user_tz":-420,"elapsed":7917,"user":{"displayName":"Hanif Nur Ilham Sanjaya","userId":"10100194631026074709"}},"outputId":"d8d8caa4-ddfa-4d95-8a72-75c138273adf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scapy\n","  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\n","Downloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scapy\n","Successfully installed scapy-2.6.1\n"]}]},{"cell_type":"code","source":["# --- 1D-CNN Data Extraction Script (Step 1) ---\n","#\n","# This script reads all .pcap files from the v2 dataset\n","# and extracts the raw packet payloads, preparing them\n","# for input into a 1D-CNN.\n","#\n","# This replaces the original 'alpha' (LSTM/packet size)\n","# and 'alpha-prime' (Bag-of-Bytes) feature extractors.\n","#\n","# It saves two files:\n","# 1. cnn_payload_data.npy: A 3D NumPy array (samples, N_PACKETS, PAYLOAD_LEN)\n","# 2. cnn_payload_labels.csv: The corresponding labels for each sample.\n","\n","print(\"--- Initializing 1D-CNN Data Extraction Script ---\")\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from scapy.all import rdpcap, TCP, UDP\n","from joblib import Parallel, delayed\n","import collections\n","\n","print(\"All libraries imported successfully.\")\n","\n","# --- PART 1: Configuration ---\n","\n","# --- Dataset Paths ---\n","BASE_PATH = \"/content/drive/MyDrive/1 Skripsi/\"\n","SOURCE_DIR = os.path.join(BASE_PATH, \"Notebook/VPNOnlyDataset\")\n","\n","# --- Output Files ---\n","OUTPUT_DATA_FILE = os.path.join(BASE_PATH, \"VPNOnly-cnn_payload_data.npy\")\n","OUTPUT_LABELS_FILE = os.path.join(BASE_PATH, \"VPNOnly-cnn_payload_labels.csv\")\n","\n","# --- CNN Parameters (from research papers) ---\n","N_PACKETS = 10     # Use the first 10 packets\n","PAYLOAD_LEN = 784  # Use the first 784 bytes of the payload\n","                   # (a common size from papers like Wang et al. [37])\n","\n","# --- KEYWORD_MAP (Copied from your previous script) ---\n","# This ensures our labels are 100% consistent\n","KEYWORD_MAP = collections.OrderedDict([\n","    ('facebook_chat', ('Facebook', 'Chat')),\n","    ('facebookchat', ('Facebook', 'Chat')),\n","    ('hangouts_chat', ('Hangout', 'Chat')),\n","    ('hangout_chat', ('Hangout', 'Chat')),\n","    ('gmailchat', ('Gmail', 'Chat')),\n","    ('icq_chat', ('ICQ', 'Chat')),\n","    ('icqchat', ('ICQ', 'Chat')),\n","    ('skype_chat', ('Skype', 'Chat')),\n","    ('aim_chat', ('AIM Chat', 'Chat')),\n","    ('aimchat', ('AIM Chat', 'Chat')),\n","    ('facebook_audio', ('Facebook', 'VoIP')),\n","    ('hangouts_audio', ('Hangout', 'VoIP')),\n","    ('skype_audio', ('Skype', 'VoIP')),\n","    ('voipbuster', ('VOIPBuster', 'VoIP')),\n","    ('facebook_video', ('Facebook', 'VoIP')),\n","    ('hangouts_video', ('Hangout', 'VoIP')),\n","    ('skype_video', ('Skype', 'VoIP')),\n","    ('skype_file', ('Skype', 'File Transfer')),\n","    ('ftps', ('FTP', 'File Transfer')),\n","    ('sftp', ('SFTP', 'File Transfer')),\n","    ('scp', ('SCP', 'File Transfer')),\n","    ('ftp', ('FTP', 'File Transfer')),\n","    ('email', ('Email', 'Email')),\n","    ('gmail', ('Gmail', 'Email')),\n","    ('netflix', ('Netflix', 'Streaming')),\n","    ('spotify', ('Spotify', 'Streaming')),\n","    ('vimeo', ('Vimeo', 'Streaming')),\n","    ('youtube', ('YouTube', 'Streaming')),\n","    ('bittorrent', ('BitTorrent', 'P2P')),\n","])\n","\n","# --- PART 2: Helper Functions ---\n","\n","def get_labels_from_filename(filename):\n","    \"\"\"\n","    Finds the application, category, and binary_type from a filename.\n","    \"\"\"\n","    lower_filename = filename.lower()\n","    binary_type = 'VPN' if lower_filename.startswith('vpn_') else 'NonVPN'\n","\n","    for keyword, (application, category) in KEYWORD_MAP.items():\n","        if keyword in lower_filename:\n","            return filename, application, category, binary_type\n","\n","    return filename, \"Unknown\", \"Unknown\", binary_type\n","\n","def process_pcap_for_cnn(pcap_filepath):\n","    \"\"\"\n","    Extracts the first N_PACKETS * PAYLOAD_LEN bytes of payload\n","    from a single pcap file and normalizes them.\n","    \"\"\"\n","    try:\n","        # Get labels\n","        filename = os.path.basename(pcap_filepath)\n","        labels = get_labels_from_filename(filename)\n","\n","        # This will be our (10, 784) array for this one flow\n","        flow_data = np.zeros((N_PACKETS, PAYLOAD_LEN), dtype=np.float32)\n","\n","        packets = rdpcap(pcap_filepath)\n","        packet_count = 0\n","\n","        for pkt in packets:\n","            if packet_count >= N_PACKETS:\n","                break\n","\n","            # Find the payload (bytes *after* TCP/UDP header)\n","            payload = None\n","            if TCP in pkt:\n","                payload = bytes(pkt[TCP].payload)\n","            elif UDP in pkt:\n","                payload = bytes(pkt[UDP].payload)\n","\n","            # Skip if no payload (e.g., pure TCP SYN) or not TCP/UDP\n","            if not payload:\n","                continue\n","\n","            payload_len = len(payload)\n","\n","            # This is our (784,) vector for this one packet\n","            normalized_payload = np.zeros(PAYLOAD_LEN, dtype=np.float32)\n","\n","            # Determine how many bytes to copy\n","            copy_len = min(payload_len, PAYLOAD_LEN)\n","\n","            # Copy the data from the byte buffer and normalize (0-255 -> 0.0-1.0)\n","            # This is a very fast, C-speed operation\n","            byte_data = np.frombuffer(payload[:copy_len], dtype=np.uint8)\n","            normalized_payload[:copy_len] = byte_data.astype(np.float32) / 255.0\n","\n","            # Add the packet vector to our flow matrix\n","            flow_data[packet_count] = normalized_payload\n","            packet_count += 1\n","\n","        # If we didn't find *any* packets with payload, skip this file\n","        if packet_count == 0:\n","            # This was changed to a print statement to avoid cluttering the log\n","            # print(f\"Skipping {filename}: No TCP/UDP payload found.\")\n","            return None\n","\n","        # Return the (10, 784) data block and the labels\n","        return flow_data, labels\n","\n","    except Exception as e:\n","        # print(f\"Error processing {pcap_filepath}: {e}\")\n","        return None\n","\n","# --- PART 3: Main Execution ---\n","def main():\n","    print(f\"Reading from: {SOURCE_DIR}\")\n","    if not os.path.exists(SOURCE_DIR):\n","        print(f\"FATAL ERROR: Source directory not found.\")\n","        return\n","\n","    filenames = [os.path.join(SOURCE_DIR, f) for f in os.listdir(SOURCE_DIR)\n","                 if f.endswith('.pcap')]\n","    print(f\"Found {len(filenames)} .pcap files to process.\")\n","\n","    # Process all files in parallel\n","    print(\"Processing files in parallel... (This may take several minutes)\")\n","    start_time = time.time()\n","    results = Parallel(n_jobs=-1, verbose=5)(\n","        delayed(process_pcap_for_cnn)(f) for f in filenames\n","    )\n","    end_time = time.time()\n","    print(f\"File processing finished in {end_time - start_time:.2f} seconds.\")\n","\n","    # Filter out empty/corrupted files\n","    valid_results = [r for r in results if r is not None]\n","\n","    print(f\"Successfully processed {len(valid_results)} files.\")\n","    print(f\"Skipped {len(filenames) - len(valid_results)} empty/corrupted/unlabeled files.\")\n","\n","    # Unzip the results into data (X) and labels (y)\n","    # X_data will be a list of (10, 784) arrays\n","    # labels_list will be a list of ('filename', 'app', 'cat', 'bin') tuples\n","    X_data_list = [r[0] for r in valid_results]\n","    labels_list = [r[1] for r in valid_results] # <-- Defined with one underscore\n","\n","    # --- Save the data ---\n","    try:\n","        # Stack all (10, 784) arrays into one big (N, 10, 784) array\n","        X_final = np.stack(X_data_list, axis=0)\n","\n","        # Save the data\n","        print(f\"Saving payload data with shape {X_final.shape} to {OUTPUT_DATA_FILE}\")\n","        np.save(OUTPUT_DATA_FILE, X_final)\n","\n","        # Save the labels\n","        # <-- FIX: Corrected variable name from 'labels__list' to 'labels_list'\n","        y_df = pd.DataFrame(labels_list, columns=['filename', 'application', 'category', 'binary_type'])\n","        print(f\"Saving labels with shape {y_df.shape} to {OUTPUT_LABELS_FILE}\")\n","        y_df.to_csv(OUTPUT_LABELS_FILE, index=False)\n","\n","        print(\"\\n--- 1D-CNN Data Extraction Finished ---\")\n","\n","    except Exception as e:\n","        print(f\"\\nFATAL ERROR during saving: {e}\")\n","        print(\"This may be a memory error. Check your Colab runtime RAM.\")\n","\n","if __name__ == \"__main__\":\n","    if not os.path.exists(\"/content/drive/MyDrive\"):\n","        print(\"Please mount your Google Drive first!\")\n","    else:\n","        main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTIuTA1e706c","outputId":"2fa5d51e-6e73-444b-d9bd-f3631af23604","executionInfo":{"status":"ok","timestamp":1762600642764,"user_tz":-420,"elapsed":418871,"user":{"displayName":"Hanif Nur Ilham Sanjaya","userId":"10100194631026074709"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Initializing 1D-CNN Data Extraction Script ---\n","All libraries imported successfully.\n","Reading from: /content/drive/MyDrive/1 Skripsi/Notebook/VPNOnlyDataset\n","Found 2730 .pcap files to process.\n","Processing files in parallel... (This may take several minutes)\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.7s\n","[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    8.1s\n","[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   17.5s\n","[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:  2.2min\n","[Parallel(n_jobs=-1)]: Done 1137 tasks      | elapsed:  2.5min\n","[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  2.6min\n","/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:  3.5min\n","[Parallel(n_jobs=-1)]: Done 2730 out of 2730 | elapsed:  7.0min finished\n"]},{"output_type":"stream","name":"stdout","text":["File processing finished in 418.56 seconds.\n","Successfully processed 2623 files.\n","Skipped 107 empty/corrupted/unlabeled files.\n","Saving payload data with shape (2623, 10, 784) to /content/drive/MyDrive/1 Skripsi/VPNOnly-cnn_payload_data.npy\n","Saving labels with shape (2623, 4) to /content/drive/MyDrive/1 Skripsi/VPNOnly-cnn_payload_labels.csv\n","\n","--- 1D-CNN Data Extraction Finished ---\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WTMcEkTuLbSj"},"execution_count":null,"outputs":[]}]}